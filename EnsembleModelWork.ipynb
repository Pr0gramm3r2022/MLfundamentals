{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4139e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn import datasets\n",
    "# import some data\n",
    "iris = datasets.load_iris()\n",
    "#retrieve the data\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8a73763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n",
      "(105, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(y_test.shape)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "287da3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 107\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Train the simplified decision tree\u001b[39;00m\n\u001b[32m    106\u001b[39m simplified_tree = DecisionTreeClassifierSimplified()\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \u001b[43msimplified_tree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_attribute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrained Decision Tree (Simplified):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mDecisionTreeClassifierSimplified.fit\u001b[39m\u001b[34m(self, df, target_attribute)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, target_attribute):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28mself\u001b[39m.tree = \u001b[38;5;28mself\u001b[39m._build_tree(df, target_attribute, \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m.drop(target_attribute).tolist())\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "#DecisionTree\n",
    "irisFeatures = iris.feature_names #Our Labels\n",
    "print(irisFeatures)\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    \"\"\"Calculates the entropy of a target variable.\"\"\"\n",
    "    hist = Counter(y)\n",
    "    probs = [count / len(y) for count in hist.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs if p > 0)\n",
    "\n",
    "def split_data(df, attribute):\n",
    "    \"\"\"Splits the dataset based on the values of a given attribute.\"\"\"\n",
    "    splits = {}\n",
    "    for value in df[attribute].unique():\n",
    "        splits[value] = df[df[attribute] == value]\n",
    "    return splits\n",
    "\n",
    "def select_best_attribute(df, target_attribute, attributes):\n",
    "    \"\"\"Selects the attribute that gives the maximum information gain.\"\"\"\n",
    "    best_gain = 0\n",
    "    best_attribute = None\n",
    "    entropy_parent = calculate_entropy(df[target_attribute])\n",
    "\n",
    "    for attribute in attributes:\n",
    "        gain = entropy_parent\n",
    "        splits = split_data(df, attribute)\n",
    "        for subset in splits.values():\n",
    "            prob = len(subset) / len(df)\n",
    "            gain -= prob * calculate_entropy(subset[target_attribute])\n",
    "\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_attribute = attribute\n",
    "    return best_attribute\n",
    "\n",
    "class DecisionTreeClassifierSimplified:\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, df, target_attribute):\n",
    "        self.tree = self._build_tree(df, target_attribute, df.columns.drop(target_attribute).tolist())\n",
    "\n",
    "    def _build_tree(self, df, target_attribute, attributes):\n",
    "        # Base cases:\n",
    "        if len(df[target_attribute].unique()) == 1:\n",
    "            return df[target_attribute].iloc[0]  # Return the single class\n",
    "\n",
    "        if not attributes:\n",
    "            return Counter(df[target_attribute]).most_common(1)[0][0] # Return majority class\n",
    "\n",
    "        # Recursive step:\n",
    "        best_attribute = select_best_attribute(df, target_attribute, attributes)\n",
    "\n",
    "        if best_attribute is None: # No information gain\n",
    "            return Counter(df[target_attribute]).most_common(1)[0][0]\n",
    "\n",
    "        tree = {best_attribute: {}}\n",
    "        remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n",
    "        splits = split_data(df, best_attribute)\n",
    "\n",
    "        for value, subset in splits.items():\n",
    "            tree[best_attribute][value] = self._build_tree(subset.copy(), target_attribute, remaining_attributes)\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def predict(self, instance):\n",
    "        def _traverse_tree(tree, instance):\n",
    "            if not isinstance(tree, dict):\n",
    "                return tree  # Leaf node\n",
    "\n",
    "            attribute = list(tree.keys())[0]\n",
    "            value = instance.get(attribute)\n",
    "            if value in tree[attribute]:\n",
    "                return _traverse_tree(tree[attribute][value], instance)\n",
    "            else:\n",
    "                # Handle unseen values (simplified: return majority of training target)\n",
    "                # A more robust approach would involve handling missing branches\n",
    "                return Counter(self._get_all_targets(self.tree)).most_common(1)[0][0]\n",
    "\n",
    "        return _traverse_tree(self.tree, instance)\n",
    "\n",
    "    def _get_all_targets(self, tree):\n",
    "        \"\"\"Helper function to extract all target values from the tree (for unseen values).\"\"\"\n",
    "        targets = []\n",
    "        if not isinstance(tree, dict):\n",
    "            return [tree]\n",
    "        for subtree in tree.values():\n",
    "            if isinstance(subtree, dict):\n",
    "                for inner_subtree in subtree.values():\n",
    "                    targets.extend(self._get_all_targets(inner_subtree))\n",
    "            else:\n",
    "                targets.append(subtree)\n",
    "        return targets\n",
    "\n",
    "# Example Usage:\n",
    "data =  X\n",
    "\n",
    "irisdf = pd.DataFrame(data)\n",
    "target_attribute = 'sepal length'\n",
    "\n",
    "# Train the simplified decision tree\n",
    "simplified_tree = DecisionTreeClassifierSimplified()\n",
    "simplified_tree.fit(irisdf, target_attribute)\n",
    "\n",
    "print(\"Trained Decision Tree (Simplified):\")\n",
    "import json\n",
    "print(json.dumps(simplified_tree.tree, indent=2))\n",
    "\n",
    "# Make a prediction\n",
    "new_instance = {'sepal_length': '6', 'Temperature': 'Mild', 'Humidity': 'High', 'Windy': 'False'}\n",
    "prediction = simplified_tree.predict(new_instance)\n",
    "print(f\"\\nPrediction for {new_instance}: {prediction}\")\n",
    "\n",
    "new_instance_unseen = {'Outlook': 'Unknown', 'Temperature': 'Cool', 'Humidity': 'Normal', 'Windy': 'True'}\n",
    "prediction_unseen = simplified_tree.predict(new_instance_unseen)\n",
    "print(f\"Prediction for unseen value {new_instance_unseen}: {prediction_unseen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cd1581c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "#DecisionTree\n",
    "irisFeatures = iris.feature_names #Our Labels\n",
    "print(irisFeatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbc06840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_counts(rows):\n",
    "    #Counts the number of each type of example in a dataset\"\n",
    "    counts = {}\n",
    "    for row in rows:\n",
    "        label = row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label] = 0\n",
    "        counts[label] += 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0e7cf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 't': 1, 'e': 3, 's': 2, 'R': 1}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts(iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f5e256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50d32e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c0b0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7dcbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7042b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d6197",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['sepal length'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# Train the simplified decision tree\u001b[39;00m\n\u001b[32m    101\u001b[39m simplified_tree = DecisionTreeClassifierSimplified()\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m \u001b[43msimplified_tree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mirisdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_attribute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrained Decision Tree:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mDecisionTreeClassifierSimplified.fit\u001b[39m\u001b[34m(self, df, target_attribute)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, df, target_attribute):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28mself\u001b[39m.tree = \u001b[38;5;28mself\u001b[39m._build_tree(df, target_attribute, \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_attribute\u001b[49m\u001b[43m)\u001b[49m.tolist())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/MLAssignments/lib/python3.12/site-packages/pandas/core/indexes/base.py:7070\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7068\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7069\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7070\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7071\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['sepal length'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    \"\"\"Calculates the entropy of a target variable.\"\"\"\n",
    "    hist = Counter(y)\n",
    "    probs = [count / len(y) for count in hist.values()]\n",
    "    return -sum(p * math.log2(p) for p in probs if p > 0)\n",
    "\n",
    "def split_data(df, attribute):\n",
    "    \"\"\"Splits the dataset based on the values of a given attribute.\"\"\"\n",
    "    splits = {}\n",
    "    for value in df[attribute].unique():\n",
    "        splits[value] = df[df[attribute] == value]\n",
    "    return splits\n",
    "\n",
    "def select_best_attribute(df, target_attribute, attributes):\n",
    "    \"\"\"Selects the attribute that gives the maximum information gain.\"\"\"\n",
    "    best_gain = 0\n",
    "    best_attribute = None\n",
    "    entropy_parent = calculate_entropy(df[target_attribute])\n",
    "\n",
    "    for attribute in attributes:\n",
    "        gain = entropy_parent\n",
    "        splits = split_data(df, attribute)\n",
    "        for subset in splits.values():\n",
    "            prob = len(subset) / len(df)\n",
    "            gain -= prob * calculate_entropy(subset[target_attribute])\n",
    "\n",
    "        if gain > best_gain:\n",
    "            best_gain = gain\n",
    "            best_attribute = attribute\n",
    "    return best_attribute\n",
    "\n",
    "class DecisionTreeClassifierSimplified:\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, df, target_attribute):\n",
    "        self.tree = self._build_tree(df, target_attribute, df.columns.drop(target_attribute).tolist())\n",
    "\n",
    "    def _build_tree(self, df, target_attribute, attributes):\n",
    "        # Base cases:\n",
    "        if len(df[target_attribute].unique()) == 1:\n",
    "            return df[target_attribute].iloc[0]  # Return the single class\n",
    "\n",
    "        if not attributes:\n",
    "            return Counter(df[target_attribute]).most_common(1)[0][0] # Return majority class\n",
    "\n",
    "        # Recursive step:\n",
    "        best_attribute = select_best_attribute(df, target_attribute, attributes)\n",
    "\n",
    "        if best_attribute is None: # No information gain\n",
    "            return Counter(df[target_attribute]).most_common(1)[0][0]\n",
    "\n",
    "        tree = {best_attribute: {}}\n",
    "        remaining_attributes = [attr for attr in attributes if attr != best_attribute]\n",
    "        splits = split_data(df, best_attribute)\n",
    "\n",
    "        for value, subset in splits.items():\n",
    "            tree[best_attribute][value] = self._build_tree(subset.copy(), target_attribute, remaining_attributes)\n",
    "\n",
    "        return tree\n",
    "\n",
    "    def predict(self, instance):\n",
    "        def _traverse_tree(tree, instance):\n",
    "            if not isinstance(tree, dict):\n",
    "                return tree  # Leaf node\n",
    "\n",
    "            attribute = list(tree.keys())[0]\n",
    "            value = instance.get(attribute)\n",
    "            if value in tree[attribute]:\n",
    "                return _traverse_tree(tree[attribute][value], instance)\n",
    "            else:\n",
    "                # Handle unseen values (simplified: return majority of training target)\n",
    "                # A more robust approach would involve handling missing branches\n",
    "                return Counter(self._get_all_targets(self.tree)).most_common(1)[0][0]\n",
    "\n",
    "        return _traverse_tree(self.tree, instance)\n",
    "\n",
    "    def _get_all_targets(self, tree):\n",
    "        \n",
    "        targets = []\n",
    "        if not isinstance(tree, dict):\n",
    "            return [tree]\n",
    "        for subtree in tree.values():\n",
    "            if isinstance(subtree, dict):\n",
    "                for inner_subtree in subtree.values():\n",
    "                    targets.extend(self._get_all_targets(inner_subtree))\n",
    "            else:\n",
    "                targets.append(subtree)\n",
    "        return targets\n",
    "\n",
    "\n",
    "data = iris\n",
    "irisdf = pd.DataFrame(X)\n",
    "target_attribute = 'sepal length'\n",
    "\n",
    "# Train the simplified decision tree\n",
    "simplified_tree = DecisionTreeClassifierSimplified()\n",
    "simplified_tree.fit(irisdf, target_attribute)\n",
    "\n",
    "print(\"Trained Decision Tree:\")\n",
    "import json\n",
    "print(json.dumps(simplified_tree.tree, indent=2))\n",
    "\n",
    "# Make a prediction\n",
    "new_instance = {'sepal length': '6', 'sepal width': '5', 'pedal length': '6', 'pedal width': '5'}\n",
    "prediction = simplified_tree.predict(new_instance)\n",
    "print(f\"\\nPrediction for {new_instance}: {prediction}\")\n",
    "\n",
    "new_instance_unseen = {'sepal length': '5', 'sepal width': '4', 'pedal length': '5', 'pedal width': '3'}\n",
    "prediction_unseen = simplified_tree.predict(new_instance_unseen)\n",
    "print(f\"Prediction for unseen value {new_instance_unseen}: {prediction_unseen}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55107951",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble Model\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "list(data.target_names)\n",
    "['malignant', 'benign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a0ae67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f684292c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date     Price\n",
      "0  2024-01-01  3.014901\n",
      "1  2024-01-02  3.005852\n",
      "2  2024-01-03  3.039431\n",
      "3  2024-01-04  3.075691\n",
      "4  2024-01-05  3.032975\n"
     ]
    }
   ],
   "source": [
    "#LSTM for Time Series Forcaseting\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "Gasdf = pd.read_csv(\"40-Day_Gas_Price_Dataset.csv\")\n",
    "print(Gasdf.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "18318f20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'float32' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m GasArray = np.asarray(Gasdf)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m CompatibleGasArray = GasArray.astype(\u001b[43mfloat32\u001b[49m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(GasArray.dtype)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#training_data = torch.Tensor(GasArray)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'float32' is not defined"
     ]
    }
   ],
   "source": [
    "GasArray = np.asarray(Gasdf)\n",
    "CompatibleGasArray = GasArray.astype(float32)\n",
    "print(GasArray.dtype)\n",
    "#training_data = torch.Tensor(GasArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192d8a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2024-01-01'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[69]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m GasArray = Gasdf.values\n\u001b[32m      5\u001b[39m GasArray2 = np.array(Gasdf)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m GasArrayFloat = \u001b[43mGasArray2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#GasTensor = torch.tensor(GasArray, dtype=torch.float32)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m'''GasArray = np.asarray(Gasdf)\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03mtraining_data = torch.Tensor(GasArray)'''\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: could not convert string to float: '2024-01-01'"
     ]
    }
   ],
   "source": [
    "batchsize=100\n",
    "#Transform the dataframe to a tensor for training. Split the ata first\n",
    "\n",
    "GasArray = Gasdf.values\n",
    "GasArray2 = np.array(Gasdf)\n",
    "GasArrayFloat = GasArray2.astype(np.float32)\n",
    "#GasTensor = torch.tensor(GasArray, dtype=torch.float32)\n",
    "'''GasArray = np.asarray(Gasdf)\n",
    "training_data = torch.Tensor(GasArray)'''\n",
    "#datasets.FashionMNIST(root=\"40-Day_Gas_Price_Dataset.csv\", train=True, transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batchsize)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batchsize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2c3f47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "37cd4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = 28\n",
    "input_len = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cb7ef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_len, hidden_size, num_class, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_len, hidden_size, num_layers, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        hidden_states = torch.zeros(self.num_layers, X.size(0), self.hidden_size)\n",
    "        cell_states = torch.zeros(self.num_layers, X.size(0), self.hidden_size)\n",
    "        out, _ = self.lstm(X, (hidden_states, cell_states))\n",
    "        out = self.output_layer(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "afc38cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(28, 128, num_layers=2, batch_first=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(input_len, hidden_size, num_classes, num_layers)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e35fbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "sgd = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "adam = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb6011c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, model, train_dataloader, loss_func, optimizer):\n",
    "    total_steps = len(train_dataloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, (images, labels) in enumerate(train_dataloader):\n",
    "            images = images.reshape(-1, sequence_len, input_len)\n",
    "            \n",
    "            output = model(images)\n",
    "            loss = loss_func(output, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (batch+1)%100 == 0:\n",
    "                print(f\"Epoch: {epoch+1}; Batch {batch+1} / {total_steps}; Loss: {loss.item():>4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce2b9a6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train(num_epochs, model, \u001b[43mtrain_dataloader\u001b[49m, loss_func, adam)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "train(num_epochs, model, train_dataloader, loss_func, adam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39101295",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[67]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test_predictions, test_labels = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mtest_dataloader\u001b[49m))\n\u001b[32m      2\u001b[39m test_labels\n",
      "\u001b[31mNameError\u001b[39m: name 'test_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "test_predictions, test_labels = next(iter(test_dataloader))\n",
    "test_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21732ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = [1 for i in range(100) if predicted[i] == test_labels[i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ec320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ensemble Model\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "list(data.target_names)\n",
    "['malignant', 'benign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff45f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Add bias term to X_train and X_test\n",
    "X_train_b = np.c_[np.ones((X_train.shape[0], 1)), X_train]\n",
    "X_test_b = np.c_[np.ones((X_test.shape[0], 1)), X_test]\n",
    "\n",
    "print(f\"Train shape: {X_train_b.shape}, Test shape: {X_test_b.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92612a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96b0b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, learning_rate=0.01, epochs=1000):\n",
    "    m, n = X.shape\n",
    "    theta = np.zeros(n)\n",
    "    loss_history = []\n",
    "    predictionVector = np.array\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        predictions = sigmoid(X @ theta)  # Compute predictions\n",
    "        gradients = (1/m) * X.T @ (predictions - y)  # Compute gradients\n",
    "        theta -= learning_rate * gradients  # Update parameters\n",
    "\n",
    "\n",
    "        loss = -np.mean(y * np.log(predictions + 1e-8) + (1 - y) * np.log(1 - predictions + 1e-8))\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "\n",
    "            print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
    "\n",
    "    return theta, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ebb89b85",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m theta_gd, loss_history = gradient_descent(\u001b[43mX_train_b\u001b[49m, y_train, learning_rate=\u001b[32m0.1\u001b[39m, epochs=\u001b[32m1000\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEstimated Coefficients:\u001b[39m\u001b[33m\"\u001b[39m, theta_gd)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_b' is not defined"
     ]
    }
   ],
   "source": [
    "theta_gd, loss_history = gradient_descent(X_train_b, y_train, learning_rate=0.1, epochs=1000)\n",
    "\n",
    "print(\"\\nEstimated Coefficients:\", theta_gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e29b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Binary Cross-Entropy Loss\")\n",
    "plt.title(\"Loss Curve for Logistic Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f76aa29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.64102564 0.43589744 0.16666667 0.01282051]\n",
      " [0.61538462 0.37179487 0.16666667 0.01282051]\n",
      " [0.58974359 0.3974359  0.15384615 0.01282051]\n",
      " [0.57692308 0.38461538 0.17948718 0.01282051]\n",
      " [0.62820513 0.44871795 0.16666667 0.01282051]\n",
      " [0.67948718 0.48717949 0.20512821 0.03846154]\n",
      " [0.57692308 0.42307692 0.16666667 0.02564103]\n",
      " [0.62820513 0.42307692 0.17948718 0.01282051]\n",
      " [0.55128205 0.35897436 0.16666667 0.01282051]\n",
      " [0.61538462 0.38461538 0.17948718 0.        ]\n",
      " [0.67948718 0.46153846 0.17948718 0.01282051]\n",
      " [0.6025641  0.42307692 0.19230769 0.01282051]\n",
      " [0.6025641  0.37179487 0.16666667 0.        ]\n",
      " [0.53846154 0.37179487 0.12820513 0.        ]\n",
      " [0.73076923 0.5        0.14102564 0.01282051]\n",
      " [0.71794872 0.55128205 0.17948718 0.03846154]\n",
      " [0.67948718 0.48717949 0.15384615 0.03846154]\n",
      " [0.64102564 0.43589744 0.16666667 0.02564103]\n",
      " [0.71794872 0.47435897 0.20512821 0.02564103]\n",
      " [0.64102564 0.47435897 0.17948718 0.02564103]\n",
      " [0.67948718 0.42307692 0.20512821 0.01282051]\n",
      " [0.64102564 0.46153846 0.17948718 0.03846154]\n",
      " [0.57692308 0.44871795 0.11538462 0.01282051]\n",
      " [0.64102564 0.41025641 0.20512821 0.05128205]\n",
      " [0.6025641  0.42307692 0.23076923 0.01282051]\n",
      " [0.62820513 0.37179487 0.19230769 0.01282051]\n",
      " [0.62820513 0.42307692 0.19230769 0.03846154]\n",
      " [0.65384615 0.43589744 0.17948718 0.01282051]\n",
      " [0.65384615 0.42307692 0.16666667 0.01282051]\n",
      " [0.58974359 0.3974359  0.19230769 0.01282051]\n",
      " [0.6025641  0.38461538 0.19230769 0.01282051]\n",
      " [0.67948718 0.42307692 0.17948718 0.03846154]\n",
      " [0.65384615 0.51282051 0.17948718 0.        ]\n",
      " [0.69230769 0.52564103 0.16666667 0.01282051]\n",
      " [0.61538462 0.38461538 0.17948718 0.01282051]\n",
      " [0.62820513 0.3974359  0.14102564 0.01282051]\n",
      " [0.69230769 0.43589744 0.15384615 0.01282051]\n",
      " [0.61538462 0.44871795 0.16666667 0.        ]\n",
      " [0.55128205 0.37179487 0.15384615 0.01282051]\n",
      " [0.64102564 0.42307692 0.17948718 0.01282051]\n",
      " [0.62820513 0.43589744 0.15384615 0.02564103]\n",
      " [0.56410256 0.28205128 0.15384615 0.02564103]\n",
      " [0.55128205 0.3974359  0.15384615 0.01282051]\n",
      " [0.62820513 0.43589744 0.19230769 0.06410256]\n",
      " [0.64102564 0.47435897 0.23076923 0.03846154]\n",
      " [0.6025641  0.37179487 0.16666667 0.02564103]\n",
      " [0.64102564 0.47435897 0.19230769 0.01282051]\n",
      " [0.57692308 0.3974359  0.16666667 0.01282051]\n",
      " [0.66666667 0.46153846 0.17948718 0.01282051]\n",
      " [0.62820513 0.41025641 0.16666667 0.01282051]\n",
      " [0.88461538 0.3974359  0.58974359 0.16666667]\n",
      " [0.80769231 0.3974359  0.56410256 0.17948718]\n",
      " [0.87179487 0.38461538 0.61538462 0.17948718]\n",
      " [0.69230769 0.28205128 0.5        0.15384615]\n",
      " [0.82051282 0.34615385 0.57692308 0.17948718]\n",
      " [0.71794872 0.34615385 0.56410256 0.15384615]\n",
      " [0.79487179 0.41025641 0.58974359 0.19230769]\n",
      " [0.61538462 0.29487179 0.41025641 0.11538462]\n",
      " [0.83333333 0.35897436 0.57692308 0.15384615]\n",
      " [0.65384615 0.33333333 0.48717949 0.16666667]\n",
      " [0.62820513 0.24358974 0.43589744 0.11538462]\n",
      " [0.74358974 0.37179487 0.52564103 0.17948718]\n",
      " [0.75641026 0.26923077 0.5        0.11538462]\n",
      " [0.76923077 0.35897436 0.58974359 0.16666667]\n",
      " [0.70512821 0.35897436 0.44871795 0.15384615]\n",
      " [0.84615385 0.38461538 0.55128205 0.16666667]\n",
      " [0.70512821 0.37179487 0.56410256 0.17948718]\n",
      " [0.73076923 0.33333333 0.51282051 0.11538462]\n",
      " [0.78205128 0.26923077 0.56410256 0.17948718]\n",
      " [0.70512821 0.30769231 0.48717949 0.12820513]\n",
      " [0.74358974 0.3974359  0.6025641  0.21794872]\n",
      " [0.76923077 0.34615385 0.5        0.15384615]\n",
      " [0.79487179 0.30769231 0.61538462 0.17948718]\n",
      " [0.76923077 0.34615385 0.58974359 0.14102564]\n",
      " [0.80769231 0.35897436 0.53846154 0.15384615]\n",
      " [0.83333333 0.37179487 0.55128205 0.16666667]\n",
      " [0.85897436 0.34615385 0.6025641  0.16666667]\n",
      " [0.84615385 0.37179487 0.62820513 0.20512821]\n",
      " [0.75641026 0.35897436 0.56410256 0.17948718]\n",
      " [0.71794872 0.32051282 0.43589744 0.11538462]\n",
      " [0.69230769 0.29487179 0.47435897 0.12820513]\n",
      " [0.69230769 0.29487179 0.46153846 0.11538462]\n",
      " [0.73076923 0.33333333 0.48717949 0.14102564]\n",
      " [0.75641026 0.33333333 0.64102564 0.19230769]\n",
      " [0.67948718 0.37179487 0.56410256 0.17948718]\n",
      " [0.75641026 0.42307692 0.56410256 0.19230769]\n",
      " [0.84615385 0.38461538 0.58974359 0.17948718]\n",
      " [0.79487179 0.28205128 0.55128205 0.15384615]\n",
      " [0.70512821 0.37179487 0.51282051 0.15384615]\n",
      " [0.69230769 0.30769231 0.5        0.15384615]\n",
      " [0.69230769 0.32051282 0.55128205 0.14102564]\n",
      " [0.76923077 0.37179487 0.57692308 0.16666667]\n",
      " [0.73076923 0.32051282 0.5        0.14102564]\n",
      " [0.62820513 0.28205128 0.41025641 0.11538462]\n",
      " [0.70512821 0.33333333 0.52564103 0.15384615]\n",
      " [0.71794872 0.37179487 0.52564103 0.14102564]\n",
      " [0.71794872 0.35897436 0.52564103 0.15384615]\n",
      " [0.78205128 0.35897436 0.53846154 0.15384615]\n",
      " [0.64102564 0.30769231 0.37179487 0.12820513]\n",
      " [0.71794872 0.34615385 0.51282051 0.15384615]\n",
      " [0.79487179 0.41025641 0.75641026 0.30769231]\n",
      " [0.73076923 0.33333333 0.64102564 0.23076923]\n",
      " [0.8974359  0.37179487 0.74358974 0.25641026]\n",
      " [0.79487179 0.35897436 0.70512821 0.21794872]\n",
      " [0.82051282 0.37179487 0.73076923 0.26923077]\n",
      " [0.96153846 0.37179487 0.83333333 0.25641026]\n",
      " [0.61538462 0.30769231 0.56410256 0.20512821]\n",
      " [0.92307692 0.35897436 0.79487179 0.21794872]\n",
      " [0.84615385 0.30769231 0.73076923 0.21794872]\n",
      " [0.91025641 0.44871795 0.76923077 0.30769231]\n",
      " [0.82051282 0.3974359  0.64102564 0.24358974]\n",
      " [0.80769231 0.33333333 0.66666667 0.23076923]\n",
      " [0.85897436 0.37179487 0.69230769 0.25641026]\n",
      " [0.71794872 0.30769231 0.62820513 0.24358974]\n",
      " [0.73076923 0.34615385 0.64102564 0.29487179]\n",
      " [0.80769231 0.3974359  0.66666667 0.28205128]\n",
      " [0.82051282 0.37179487 0.69230769 0.21794872]\n",
      " [0.97435897 0.47435897 0.84615385 0.26923077]\n",
      " [0.97435897 0.32051282 0.87179487 0.28205128]\n",
      " [0.75641026 0.26923077 0.62820513 0.17948718]\n",
      " [0.87179487 0.3974359  0.71794872 0.28205128]\n",
      " [0.70512821 0.34615385 0.61538462 0.24358974]\n",
      " [0.97435897 0.34615385 0.84615385 0.24358974]\n",
      " [0.79487179 0.33333333 0.61538462 0.21794872]\n",
      " [0.84615385 0.41025641 0.71794872 0.25641026]\n",
      " [0.91025641 0.3974359  0.75641026 0.21794872]\n",
      " [0.78205128 0.34615385 0.6025641  0.21794872]\n",
      " [0.76923077 0.37179487 0.61538462 0.21794872]\n",
      " [0.80769231 0.34615385 0.70512821 0.25641026]\n",
      " [0.91025641 0.37179487 0.73076923 0.19230769]\n",
      " [0.93589744 0.34615385 0.76923077 0.23076923]\n",
      " [1.         0.47435897 0.80769231 0.24358974]\n",
      " [0.80769231 0.34615385 0.70512821 0.26923077]\n",
      " [0.79487179 0.34615385 0.64102564 0.17948718]\n",
      " [0.76923077 0.32051282 0.70512821 0.16666667]\n",
      " [0.97435897 0.37179487 0.76923077 0.28205128]\n",
      " [0.79487179 0.42307692 0.70512821 0.29487179]\n",
      " [0.80769231 0.38461538 0.69230769 0.21794872]\n",
      " [0.75641026 0.37179487 0.6025641  0.21794872]\n",
      " [0.87179487 0.38461538 0.67948718 0.25641026]\n",
      " [0.84615385 0.38461538 0.70512821 0.29487179]\n",
      " [0.87179487 0.38461538 0.64102564 0.28205128]\n",
      " [0.73076923 0.33333333 0.64102564 0.23076923]\n",
      " [0.85897436 0.3974359  0.74358974 0.28205128]\n",
      " [0.84615385 0.41025641 0.71794872 0.30769231]\n",
      " [0.84615385 0.37179487 0.65384615 0.28205128]\n",
      " [0.79487179 0.30769231 0.62820513 0.23076923]\n",
      " [0.82051282 0.37179487 0.65384615 0.24358974]\n",
      " [0.78205128 0.42307692 0.67948718 0.28205128]\n",
      " [0.74358974 0.37179487 0.64102564 0.21794872]]\n"
     ]
    }
   ],
   "source": [
    "def normalize(df):\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "    normdf = min_max_normalize(df)\n",
    "    normdf.to_csv('normalized_data.csv', index=False)\n",
    "\n",
    "normdf = normalize(data)\n",
    "print(normdf)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adaff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = normdf[0:134]\n",
    "print(trainSet)\n",
    "trainArray = trainSet.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9b3907fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(y_test.shape)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "645c0bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#KNN function\n",
    "\n",
    "class KNN:\n",
    "  def __init__(self,k):\n",
    "    self.k = k\n",
    "    self.trainingData = None;\n",
    "    self.trainingLabels = None;\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    self.training_data = X\n",
    "    self.training_labels = y\n",
    "\n",
    "  def euclidean_distance(p, q):\n",
    "    return np.sqrt(np.sum((np.array(p) - np.array(q)) ** 2))\n",
    "\n",
    "\n",
    "  def predict(self, X_test):\n",
    "    predictions = []\n",
    "    for test_point in X_test:\n",
    "      distances = []\n",
    "\n",
    "      for i, training_point in enumerate(self.training_data):\n",
    "        #iterates through the test sets and calculates the euclidean distance between the 2 points\n",
    "        distance = euclidean_distance(test_point, training_point)\n",
    "        distances.append((distance, i))\n",
    "        #storing distances as a tuple\n",
    "        #i is each pair of points. ex: point 0 is (training[0], test[0], and point 2 and point 3 and so on)\n",
    "        #using the lambda funtion to sort the list by the first element of each point which is the distance(distance, i)\n",
    "        distances.sort(key=lambda x: x[0])\n",
    "\n",
    "        k_nearest_indices = [index for (_, index) in distances[:self.k]]\n",
    "        k_nearest = distances[:self.k]\n",
    "        #neighbor_labels = [self.training_labels[i] for i in k_nearest]\n",
    "        neighbor_labels = self.training_labels[k_nearest_indices]\n",
    "        '''prediction = max(set(neighbor_labels), key=neighbor_labels.count)\n",
    "        predictions.append(prediction)'''\n",
    "\n",
    "        most_common_label = Counter(neighbor_labels).most_common(1)[0][0]\n",
    "        predictions.append(most_common_label)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5435456a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'euclidean_distance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m knn = KNN(k=\u001b[32m1\u001b[39m)\n\u001b[32m      3\u001b[39m knn.fit(X_train, y_train)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m predictions = \u001b[43mknn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#this line of code is giving errors and I have no idea why.\u001b[39;00m\n\u001b[32m      8\u001b[39m accuracy = accuracy_score(y_test, predictions)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mKNN.predict\u001b[39m\u001b[34m(self, X_test)\u001b[39m\n\u001b[32m     21\u001b[39m distances = []\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, training_point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.training_data):\n\u001b[32m     24\u001b[39m   \u001b[38;5;66;03m#iterates through the test sets and calculates the euclidean distance between the 2 points\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m   distance = \u001b[43meuclidean_distance\u001b[49m(test_point, training_point)\n\u001b[32m     26\u001b[39m   distances.append((distance, i))\n\u001b[32m     27\u001b[39m   \u001b[38;5;66;03m#storing distances as a tuple\u001b[39;00m\n\u001b[32m     28\u001b[39m   \u001b[38;5;66;03m#i is each pair of points. ex: point 0 is (training[0], test[0], and point 2 and point 3 and so on)\u001b[39;00m\n\u001b[32m     29\u001b[39m   \u001b[38;5;66;03m#using the lambda funtion to sort the list by the first element of each point which is the distance(distance, i)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'euclidean_distance' is not defined"
     ]
    }
   ],
   "source": [
    "X, y = load_breast_cancer().data, load_breast_cancer().target\n",
    "knn = KNN(k=1)\n",
    "knn.fit(X_train, y_train)\n",
    "predictions = knn.predict(X_test)\n",
    "\n",
    "\n",
    "#this line of code is giving errors and I have no idea why.\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "\n",
    "class_names = columnNames\n",
    "titles_options = [\n",
    "    (\"Confusion matrix, without normalization\", None),\n",
    "    (\"Normalized confusion matrix\", \"true\"),\n",
    "]\n",
    "\n",
    "\n",
    "for titles, normalize in titles_options:\n",
    "    display = ConfusionMatrixDisplay.from_predictions(\n",
    "        y_test, predictions,  # Pass true labels (y_test) and predictions\n",
    "        display_labels=class_names,\n",
    "        cmap=plt.cm.inferno,\n",
    "        normalize=normalize,\n",
    "    )\n",
    "    display.ax_.set_title(titles)\n",
    "    print(titles)\n",
    "    print(display.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLAssignments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
